{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of\n",
    "discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each\n",
    "item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in\n",
    "turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of\n",
    "text modeling, the topic probabilities provide an explicit representation of a document. Read more in <a href=\"http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\">\"Latent Dirichlet Allocation\" paper. </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every NLP model has 3 sections that have to be fullfiled:\n",
    "<ol>\n",
    "  <li>Text Processing </li>\n",
    "  <li>Feature Extraction</li>\n",
    "  <li>Modeling</li>\n",
    "</ol>\n",
    "Below you can follow these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the text processing section we do the following:\n",
    "\n",
    "<ol>\n",
    "  <li>**Tokenization:** Splitting the text into words + Lowercasing the words + Deleting the punctuation. </li>\n",
    "  <li>**Eliminating irelavants:** Words with less than 3 alphabets + Stopwords. </li>\n",
    "  <li>**Lemmatization:** Converting third person to first person + Converting every verb time to persent. </li>\n",
    "  <li>**Stemming:** Finding the root of each word.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Seed\n",
    "np.random.seed(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv(filepath_or_buffer = \"./abcnews-date-text.csv\", error_bad_lines = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the second column\n",
    "data_text = data[:300000][[\"headline_text\"]]\n",
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding a 'index' column\n",
    "data_text['index'] = data_text.index\n",
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  300000\n"
     ]
    }
   ],
   "source": [
    "# Making our documents\n",
    "documents = data_text\n",
    "\n",
    "# Total number of documents\n",
    "print(\"Total number of documents: \", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Preprocessing the Raw Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the NLP libraries\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/soheil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization and Stemming\n",
    "def lemmatize_stemming(text):\n",
    "    \"\"\"\n",
    "    Applying the lemmatization and stemming to the give text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : Raw text for lemmatization and stemming\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Lemmatized and stemmed text\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initializing the stemmer\n",
    "    stemmer = SnowballStemmer(language = 'english')\n",
    "    \n",
    "    # Applying lemmatization\n",
    "    lemmatized_word = WordNetLemmatizer().lemmatize(word = text, pos = 'v')\n",
    "    \n",
    "    # Applying stemming\n",
    "    stemmed_word = stemmer.stem(word = lemmatized_word)\n",
    "    \n",
    "    return stemmed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocessing the text, including: lowercasing, eliminating punctuation, eliminating words less than 3,\n",
    "    eliminating the stopwords, and at the end applying the lemmatization and stemming.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : Raw text to apply preprocessing\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Preprocessed text\n",
    "    \n",
    "    \"\"\"\n",
    "    # initializing any empty list for the results\n",
    "    result = []\n",
    "    \n",
    "    # Iterating through tokens + Lowercasing and eliminating the punctuation\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        \n",
    "        # Eliminating stopwords \n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "            \n",
    "            # Eliminating words less than 3\n",
    "            if len(token)>3:\n",
    "                \n",
    "                # Applying lemmatization and stemming\n",
    "                lemm_stem_word = lemmatize_stemming(text = token)\n",
    "                \n",
    "                # Appending the preprocessed word\n",
    "                result.append(lemm_stem_word)\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Sample Document:  australia to contribute 10 million in aid to iraq\n",
      "==> Original Words:  ['australia', 'to', 'contribute', '10', 'million', 'in', 'aid', 'to', 'iraq']\n",
      "==> Preprocessed Words:  ['australia', 'contribut', 'million', 'iraq']\n"
     ]
    }
   ],
   "source": [
    "document_number = 10\n",
    "\n",
    "# Sampling a document \n",
    "sample_doc = documents[documents['index'] == document_number].values[0][0]\n",
    "print(\"==> Sample Document: \", sample_doc)\n",
    "\n",
    "# Tokenizing the sample\n",
    "sample_words = [s for s in sample_doc.split(' ')]\n",
    "print(\"==> Original Words: \", sample_words)\n",
    "\n",
    "# Preprocessing the sample\n",
    "preprocessed_words = preprocess(sample_doc)\n",
    "print(\"==> Preprocessed Words: \", preprocessed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the whole document\n",
    "processed_documents = documents['headline_text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [decid, communiti, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_documents.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dictionary cotaining words and their integer ids\n",
    "dictionary = gensim.corpora.Dictionary(processed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n"
     ]
    }
   ],
   "source": [
    "# Printing first 10 items in dictionary\n",
    "count = 0\n",
    "for key, value in dictionary.iteritems():\n",
    "    print(key, value)\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletting very rare and very common words\n",
    "dictionary.filter_extremes(no_below = 15, # Removing words with less than 15 (absolute number)\n",
    "                           no_above = 0.1, # Words appearing more than 10% of documents (fraction of size, not absolute number)\n",
    "                           keep_n = 100000) # keeping the most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words of our sample document:  [(34, 1), (36, 1), (39, 1), (40, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Applying Bag-of-Words for each document (a list of words)\n",
    "bow_corpus = [dictionary.doc2bow(document = doc) for doc in processed_documents]\n",
    "\n",
    "print(\"Bag-of-Words of our sample document: \", bow_corpus[document_number]) # document_number = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"iraq\" with word id of 34 repeated 1 times\n",
      "The word \"australia\" with word id of 36 repeated 1 times\n",
      "The word \"contribut\" with word id of 39 repeated 1 times\n",
      "The word \"million\" with word id of 40 repeated 1 times\n"
     ]
    }
   ],
   "source": [
    "# Printing out the BoW for our sample document\n",
    "bow_10 = bow_corpus[document_number]\n",
    "\n",
    "for i in range(len(bow_10)):\n",
    "    print('The word \"{}\" with word id of {} repeated {} times'.format(dictionary[bow_10[i][0]],\n",
    "                                                                    bow_10[i][0],\n",
    "                                                                    bow_10[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. TF-IDF (Term Frequency, Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TF-IDF model\n",
    "tdidf = models.TfidfModel(corpus = bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st TD-IDF:  [(0, 0.5959813347777092), (1, 0.39204529549491984), (2, 0.48531419274988147), (3, 0.5055461098578569)]\n"
     ]
    }
   ],
   "source": [
    "# Applying TF-IDF to the entire corpus\n",
    "tdidf_corpus = tdidf[bow_corpus]\n",
    "\n",
    "print(\"1st TD-IDF: \", tdidf_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5959813347777092),\n",
      " (1, 0.39204529549491984),\n",
      " (2, 0.48531419274988147),\n",
      " (3, 0.5055461098578569)]\n"
     ]
    }
   ],
   "source": [
    "# Preview TF-IDF for first document\n",
    "for doc in tdidf_corpus:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Runnung LDA (Latent Dirichlet Allocation) Using Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a LDA model\n",
    "# LdaModel = Mono core /  LdaMulticore = Multi core\n",
    "lda_model_bow = gensim.models.LdaMulticore(corpus = bow_corpus,\n",
    "                                       num_topics = 10,\n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2,  # Iteration\n",
    "                                       workers = 2) # Using 2 processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*\"lead\" + 0.021*\"open\" + 0.017*\"take\" + 0.014*\"aussi\" + 0.013*\"test\" + 0.013*\"england\" + 0.012*\"injur\" + 0.011*\"victori\" + 0.011*\"vote\" + 0.010*\"unit\" \n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.055*\"polic\" + 0.028*\"death\" + 0.023*\"investig\" + 0.022*\"continu\" + 0.022*\"miss\" + 0.016*\"price\" + 0.014*\"search\" + 0.014*\"probe\" + 0.014*\"road\" + 0.012*\"warn\" \n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.040*\"charg\" + 0.035*\"court\" + 0.033*\"face\" + 0.022*\"accus\" + 0.021*\"jail\" + 0.021*\"murder\" + 0.020*\"polic\" + 0.016*\"case\" + 0.013*\"trial\" + 0.013*\"world\" \n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.044*\"govt\" + 0.023*\"fund\" + 0.021*\"urg\" + 0.014*\"health\" + 0.013*\"labor\" + 0.013*\"seek\" + 0.013*\"opposit\" + 0.013*\"servic\" + 0.012*\"help\" + 0.012*\"boost\" \n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.028*\"kill\" + 0.024*\"crash\" + 0.023*\"attack\" + 0.021*\"drug\" + 0.017*\"die\" + 0.015*\"nuclear\" + 0.013*\"dead\" + 0.012*\"strike\" + 0.011*\"north\" + 0.011*\"guilti\" \n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.035*\"say\" + 0.020*\"farmer\" + 0.020*\"drought\" + 0.016*\"howard\" + 0.016*\"deal\" + 0.015*\"talk\" + 0.015*\"iraq\" + 0.013*\"final\" + 0.013*\"rain\" + 0.010*\"trade\" \n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.050*\"plan\" + 0.033*\"water\" + 0.032*\"council\" + 0.017*\"govt\" + 0.015*\"closer\" + 0.014*\"group\" + 0.014*\"work\" + 0.013*\"push\" + 0.012*\"consid\" + 0.012*\"industri\" \n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.023*\"concern\" + 0.015*\"power\" + 0.014*\"close\" + 0.013*\"firefight\" + 0.013*\"centr\" + 0.012*\"resid\" + 0.012*\"blaze\" + 0.011*\"water\" + 0.010*\"battl\" + 0.010*\"compani\" \n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.022*\"australia\" + 0.020*\"aust\" + 0.018*\"return\" + 0.015*\"play\" + 0.012*\"announc\" + 0.012*\"hick\" + 0.010*\"sign\" + 0.010*\"record\" + 0.009*\"challeng\" + 0.008*\"doubt\" \n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.019*\"win\" + 0.019*\"arrest\" + 0.018*\"rule\" + 0.017*\"protest\" + 0.017*\"polic\" + 0.016*\"bomb\" + 0.014*\"offic\" + 0.013*\"target\" + 0.013*\"kill\" + 0.012*\"terror\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring words in each topic and it weight\n",
    "for index, topic in lda_model_bow.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(index, topic), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Running LDA (Latent Dirichlet Allocation) Using TF-IDF (Term Frequency, Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LDA model\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus = bow_corpus, \n",
    "                                       num_topics = 10, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2,\n",
    "                                       workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*\"govt\" + 0.019*\"urg\" + 0.018*\"defend\" + 0.016*\"indigen\" + 0.016*\"driver\" + 0.015*\"rise\" + 0.015*\"public\" + 0.015*\"council\" + 0.015*\"hold\" + 0.014*\"road\" \n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.023*\"hospit\" + 0.020*\"worker\" + 0.016*\"centr\" + 0.013*\"power\" + 0.012*\"storm\" + 0.011*\"threat\" + 0.011*\"delay\" + 0.010*\"govt\" + 0.010*\"action\" + 0.009*\"plan\" \n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.027*\"say\" + 0.025*\"govt\" + 0.024*\"call\" + 0.020*\"labor\" + 0.015*\"howard\" + 0.013*\"deni\" + 0.013*\"inquiri\" + 0.012*\"rule\" + 0.011*\"sale\" + 0.010*\"chief\" \n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.057*\"water\" + 0.023*\"nation\" + 0.022*\"opposit\" + 0.017*\"elect\" + 0.017*\"win\" + 0.012*\"resid\" + 0.011*\"reject\" + 0.011*\"terror\" + 0.011*\"plan\" + 0.010*\"river\" \n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.039*\"kill\" + 0.020*\"iraq\" + 0.015*\"attack\" + 0.015*\"aust\" + 0.014*\"troop\" + 0.013*\"final\" + 0.012*\"open\" + 0.011*\"die\" + 0.010*\"clash\" + 0.010*\"bomb\" \n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.035*\"crash\" + 0.018*\"lead\" + 0.017*\"world\" + 0.012*\"take\" + 0.011*\"aussi\" + 0.011*\"timor\" + 0.009*\"rescu\" + 0.009*\"highway\" + 0.009*\"victori\" + 0.008*\"unit\" \n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.075*\"polic\" + 0.034*\"charg\" + 0.029*\"court\" + 0.028*\"face\" + 0.021*\"investig\" + 0.020*\"death\" + 0.020*\"jail\" + 0.017*\"murder\" + 0.017*\"drug\" + 0.015*\"accus\" \n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.028*\"fund\" + 0.021*\"boost\" + 0.017*\"govt\" + 0.016*\"farmer\" + 0.016*\"drought\" + 0.015*\"health\" + 0.014*\"price\" + 0.013*\"probe\" + 0.012*\"break\" + 0.012*\"school\" \n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.033*\"plan\" + 0.020*\"council\" + 0.018*\"concern\" + 0.017*\"back\" + 0.016*\"group\" + 0.013*\"blaze\" + 0.013*\"green\" + 0.013*\"fear\" + 0.013*\"nuclear\" + 0.012*\"appeal\" \n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.027*\"miss\" + 0.023*\"closer\" + 0.021*\"australia\" + 0.018*\"search\" + 0.015*\"england\" + 0.015*\"south\" + 0.012*\"continu\" + 0.012*\"test\" + 0.012*\"west\" + 0.011*\"trade\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exploring words in each topic and it weight\n",
    "for index, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(index, topic), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Classifying Sample Document Using Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain', 'help', 'dampen', 'bushfir']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text of sample document 4310\n",
    "document_number = 4310\n",
    "processed_documents[document_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4195842742919922 \n",
      "Topic: 0.035*\"say\" + 0.020*\"farmer\" + 0.020*\"drought\" + 0.016*\"howard\" + 0.016*\"deal\" + 0.015*\"talk\" + 0.015*\"iraq\" + 0.013*\"final\" + 0.013*\"rain\" + 0.010*\"trade\"\n",
      "\n",
      "Score: 0.22040049731731415 \n",
      "Topic: 0.028*\"kill\" + 0.024*\"crash\" + 0.023*\"attack\" + 0.021*\"drug\" + 0.017*\"die\" + 0.015*\"nuclear\" + 0.013*\"dead\" + 0.012*\"strike\" + 0.011*\"north\" + 0.011*\"guilti\"\n",
      "\n",
      "Score: 0.2199997901916504 \n",
      "Topic: 0.055*\"polic\" + 0.028*\"death\" + 0.023*\"investig\" + 0.022*\"continu\" + 0.022*\"miss\" + 0.016*\"price\" + 0.014*\"search\" + 0.014*\"probe\" + 0.014*\"road\" + 0.012*\"warn\"\n",
      "\n",
      "Score: 0.02001262456178665 \n",
      "Topic: 0.044*\"govt\" + 0.023*\"fund\" + 0.021*\"urg\" + 0.014*\"health\" + 0.013*\"labor\" + 0.013*\"seek\" + 0.013*\"opposit\" + 0.013*\"servic\" + 0.012*\"help\" + 0.012*\"boost\"\n",
      "\n",
      "Score: 0.0200028233230114 \n",
      "Topic: 0.050*\"plan\" + 0.033*\"water\" + 0.032*\"council\" + 0.017*\"govt\" + 0.015*\"closer\" + 0.014*\"group\" + 0.014*\"work\" + 0.013*\"push\" + 0.012*\"consid\" + 0.012*\"industri\"\n",
      "\n",
      "Score: 0.020000021904706955 \n",
      "Topic: 0.023*\"concern\" + 0.015*\"power\" + 0.014*\"close\" + 0.013*\"firefight\" + 0.013*\"centr\" + 0.012*\"resid\" + 0.012*\"blaze\" + 0.011*\"water\" + 0.010*\"battl\" + 0.010*\"compani\"\n",
      "\n",
      "Score: 0.02000000886619091 \n",
      "Topic: 0.019*\"win\" + 0.019*\"arrest\" + 0.018*\"rule\" + 0.017*\"protest\" + 0.017*\"polic\" + 0.016*\"bomb\" + 0.014*\"offic\" + 0.013*\"target\" + 0.013*\"kill\" + 0.012*\"terror\"\n",
      "\n",
      "Score: 0.020000001415610313 \n",
      "Topic: 0.022*\"lead\" + 0.021*\"open\" + 0.017*\"take\" + 0.014*\"aussi\" + 0.013*\"test\" + 0.013*\"england\" + 0.012*\"injur\" + 0.011*\"victori\" + 0.011*\"vote\" + 0.010*\"unit\"\n",
      "\n",
      "Score: 0.020000001415610313 \n",
      "Topic: 0.040*\"charg\" + 0.035*\"court\" + 0.033*\"face\" + 0.022*\"accus\" + 0.021*\"jail\" + 0.021*\"murder\" + 0.020*\"polic\" + 0.016*\"case\" + 0.013*\"trial\" + 0.013*\"world\"\n",
      "\n",
      "Score: 0.020000001415610313 \n",
      "Topic: 0.022*\"australia\" + 0.020*\"aust\" + 0.018*\"return\" + 0.015*\"play\" + 0.012*\"announc\" + 0.012*\"hick\" + 0.010*\"sign\" + 0.010*\"record\" + 0.009*\"challeng\" + 0.008*\"doubt\"\n"
     ]
    }
   ],
   "source": [
    "# Checking which topic does our document belongs to\n",
    "for index, score in sorted(lda_model_bow[bow_corpus[document_number]], key = lambda tup: (-1)*tup[1]):\n",
    "    print(\"\\nScore: {} \\nTopic: {}\".format(score, lda_model_bow.print_topic(topicno = index, topn = 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Classifying Sample Document Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.43032628297805786 \n",
      "Topic: 0.027*\"say\" + 0.025*\"govt\" + 0.024*\"call\" + 0.020*\"labor\" + 0.015*\"howard\" + 0.013*\"deni\" + 0.013*\"inquiri\" + 0.012*\"rule\" + 0.011*\"sale\" + 0.010*\"chief\"\n",
      "\n",
      "Score: 0.4096580445766449 \n",
      "Topic: 0.028*\"fund\" + 0.021*\"boost\" + 0.017*\"govt\" + 0.016*\"farmer\" + 0.016*\"drought\" + 0.015*\"health\" + 0.014*\"price\" + 0.013*\"probe\" + 0.012*\"break\" + 0.012*\"school\"\n",
      "\n",
      "Score: 0.020008672028779984 \n",
      "Topic: 0.057*\"water\" + 0.023*\"nation\" + 0.022*\"opposit\" + 0.017*\"elect\" + 0.017*\"win\" + 0.012*\"resid\" + 0.011*\"reject\" + 0.011*\"terror\" + 0.011*\"plan\" + 0.010*\"river\"\n",
      "\n",
      "Score: 0.020002365112304688 \n",
      "Topic: 0.023*\"hospit\" + 0.020*\"worker\" + 0.016*\"centr\" + 0.013*\"power\" + 0.012*\"storm\" + 0.011*\"threat\" + 0.011*\"delay\" + 0.010*\"govt\" + 0.010*\"action\" + 0.009*\"plan\"\n",
      "\n",
      "Score: 0.02000197395682335 \n",
      "Topic: 0.022*\"govt\" + 0.019*\"urg\" + 0.018*\"defend\" + 0.016*\"indigen\" + 0.016*\"driver\" + 0.015*\"rise\" + 0.015*\"public\" + 0.015*\"council\" + 0.015*\"hold\" + 0.014*\"road\"\n",
      "\n",
      "Score: 0.02000092715024948 \n",
      "Topic: 0.075*\"polic\" + 0.034*\"charg\" + 0.029*\"court\" + 0.028*\"face\" + 0.021*\"investig\" + 0.020*\"death\" + 0.020*\"jail\" + 0.017*\"murder\" + 0.017*\"drug\" + 0.015*\"accus\"\n",
      "\n",
      "Score: 0.020000841468572617 \n",
      "Topic: 0.033*\"plan\" + 0.020*\"council\" + 0.018*\"concern\" + 0.017*\"back\" + 0.016*\"group\" + 0.013*\"blaze\" + 0.013*\"green\" + 0.013*\"fear\" + 0.013*\"nuclear\" + 0.012*\"appeal\"\n",
      "\n",
      "Score: 0.0200006403028965 \n",
      "Topic: 0.027*\"miss\" + 0.023*\"closer\" + 0.021*\"australia\" + 0.018*\"search\" + 0.015*\"england\" + 0.015*\"south\" + 0.012*\"continu\" + 0.012*\"test\" + 0.012*\"west\" + 0.011*\"trade\"\n",
      "\n",
      "Score: 0.020000288262963295 \n",
      "Topic: 0.035*\"crash\" + 0.018*\"lead\" + 0.017*\"world\" + 0.012*\"take\" + 0.011*\"aussi\" + 0.011*\"timor\" + 0.009*\"rescu\" + 0.009*\"highway\" + 0.009*\"victori\" + 0.008*\"unit\"\n",
      "\n",
      "Score: 0.020000005140900612 \n",
      "Topic: 0.039*\"kill\" + 0.020*\"iraq\" + 0.015*\"attack\" + 0.015*\"aust\" + 0.014*\"troop\" + 0.013*\"final\" + 0.012*\"open\" + 0.011*\"die\" + 0.010*\"clash\" + 0.010*\"bomb\"\n"
     ]
    }
   ],
   "source": [
    "# Checking which topic does our document belongs to\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[document_number]], key = lambda tup: (-1)*tup[1]):\n",
    "    print(\"\\nScore: {} \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(topicno = index, topn = 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Testing the model on the onseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_document = \"My favorite sports activities are running and swimming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text:  ['favorit', 'sport', 'activ', 'run', 'swim']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the new document\n",
    "processed_text = preprocess(text = unseen_document)\n",
    "print(\"Processed text: \", processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words for given text:  [(372, 1), (1001, 1), (1632, 1), (2573, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Creating a Bag-of-Word\n",
    "bow_vector = dictionary.doc2bow(document = processed_text)\n",
    "print(\"Bag-of-Words for given text: \", bow_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5976608991622925 Topic: 0.023*\"concern\" + 0.015*\"power\" + 0.014*\"close\" + 0.013*\"firefight\" + 0.013*\"centr\"\n",
      "\n",
      "Score: 0.24232117831707 Topic: 0.022*\"lead\" + 0.021*\"open\" + 0.017*\"take\" + 0.014*\"aussi\" + 0.013*\"test\"\n",
      "\n",
      "Score: 0.020006127655506134 Topic: 0.022*\"australia\" + 0.020*\"aust\" + 0.018*\"return\" + 0.015*\"play\" + 0.012*\"announc\"\n",
      "\n",
      "Score: 0.020003166049718857 Topic: 0.050*\"plan\" + 0.033*\"water\" + 0.032*\"council\" + 0.017*\"govt\" + 0.015*\"closer\"\n",
      "\n",
      "Score: 0.020002849400043488 Topic: 0.055*\"polic\" + 0.028*\"death\" + 0.023*\"investig\" + 0.022*\"continu\" + 0.022*\"miss\"\n",
      "\n",
      "Score: 0.020002581179142 Topic: 0.019*\"win\" + 0.019*\"arrest\" + 0.018*\"rule\" + 0.017*\"protest\" + 0.017*\"polic\"\n",
      "\n",
      "Score: 0.020002515986561775 Topic: 0.040*\"charg\" + 0.035*\"court\" + 0.033*\"face\" + 0.022*\"accus\" + 0.021*\"jail\"\n",
      "\n",
      "Score: 0.020000645890831947 Topic: 0.028*\"kill\" + 0.024*\"crash\" + 0.023*\"attack\" + 0.021*\"drug\" + 0.017*\"die\"\n",
      "\n",
      "Score: 0.019999999552965164 Topic: 0.044*\"govt\" + 0.023*\"fund\" + 0.021*\"urg\" + 0.014*\"health\" + 0.013*\"labor\"\n",
      "\n",
      "Score: 0.019999999552965164 Topic: 0.035*\"say\" + 0.020*\"farmer\" + 0.020*\"drought\" + 0.016*\"howard\" + 0.016*\"deal\"\n"
     ]
    }
   ],
   "source": [
    "# Checking which topic does our document belongs to\n",
    "for index, score in sorted(lda_model_bow[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {} Topic: {}\".format(score, lda_model_bow.print_topic(topicno = index, topn = 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> News :\n",
      " big plan to boost paroo water supplies \n",
      "\n",
      "=> Topics: \n",
      "Score: 0.6207208633422852\t Topic: 0.057*\"water\"\n",
      "Score: 0.21926729381084442\t Topic: 0.028*\"fund\"\n",
      "Score: 0.020007101818919182\t Topic: 0.033*\"plan\"\n",
      "Score: 0.020002758130431175\t Topic: 0.022*\"govt\"\n",
      "Score: 0.02000199817121029\t Topic: 0.023*\"hospit\"\n",
      "Score: 0.020000003278255463\t Topic: 0.027*\"miss\"\n",
      "Score: 0.020000001415610313\t Topic: 0.027*\"say\"\n",
      "Score: 0.020000001415610313\t Topic: 0.039*\"kill\"\n",
      "Score: 0.020000001415610313\t Topic: 0.035*\"crash\"\n",
      "Score: 0.020000001415610313\t Topic: 0.075*\"polic\"\n"
     ]
    }
   ],
   "source": [
    "index = 14\n",
    "\n",
    "doc = documents[documents['index'] == index]\n",
    "doc = np.array(doc)[0][0]\n",
    "\n",
    "print(\"=> News :\\n\", doc, \"\\n\")\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(doc))\n",
    "print(\"=> Topics: \")\n",
    "for index, score in (sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1])):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
